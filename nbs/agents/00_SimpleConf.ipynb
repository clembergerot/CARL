{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85685c1",
   "metadata": {},
   "source": [
    "# SimpleConf\n",
    "\n",
    "> Performs updating of state-action values according to a simple Rescorla-Wagner rule, with use of social information. Allows for asymmetric learning rates in the form of a confirmation/disconfirmation bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e558e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents/SimpleConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "\n",
    "# Imports for the examples\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec595d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SimpleConf(object):\n",
    "    \"\"\"\n",
    "    Class for simple reinforcement learning (Rescorla-Wagner rule)\n",
    "    with confirmation/disconfirmation bias.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 params: np.ndarray):  # agents' parameters\n",
    "        \n",
    "        self.N = np.shape(params)[0]  # number of agents\n",
    "        self.M = 2  # number of options\n",
    "        self.alphac = params[:, 0]   # confirmatory learning rates\n",
    "        self.alphad = params[:, 1]   # disconfirmatory learning rates\n",
    "        self.beta = params[:, 2]    # inverse temperatures\n",
    "\n",
    "    def connect_agents_full(self):\n",
    "        \"\"\"Connects agents according to a fully connected graph.\"\"\"\n",
    "        return nx.complete_graph(self.N)\n",
    "\n",
    "    def compute_softmax(self, Qtable):\n",
    "        \"\"\"Returns a probability table for all agents for all actions,\n",
    "        from agents' Qtable.\"\"\"\n",
    "        beta = np.row_stack(self.beta)\n",
    "        num = np.exp(beta*Qtable)  # numerator\n",
    "        den = np.sum(num, axis=1)  # denominator\n",
    "        return num/den[:, None]\n",
    "\n",
    "    def choose(self, Ptable):\n",
    "        \"\"\"Computes chosen options from agents's probability table.\"\"\"\n",
    "        choices = np.zeros((np.shape(Ptable)[0]))  # 1 choice per agent\n",
    "        rd = np.reshape(np.random.rand(len(choices)), (len(choices), 1))\n",
    "        choices = np.sum(rd > np.cumsum(Ptable, axis=1), axis=1)\n",
    "        choices = choices.astype(int)  # converts the choices to int values\n",
    "        return choices\n",
    "    \n",
    "    def all_take_action(self, Qtable):\n",
    "        \"\"\"Computes all agents' choices from their Qtable.\n",
    "        Combines `compute_softmax` and `choose`.\n",
    "        \"\"\"\n",
    "        Ptable = self.compute_softmax(Qtable)\n",
    "        choices = self.choose(Ptable)\n",
    "        return choices\n",
    "    \n",
    "    def update_Qvalues(self, G_att, choices, payoffs, Qtable): \n",
    "        \"\"\"Updates all agents' Q-values according to CARL, \n",
    "        without for loops.\"\"\"\n",
    "        Qs = np.einsum('ijk->ikj', np.reshape(np.repeat(Qtable, self.N), \n",
    "                                              (self.N, self.M, self.N)))\n",
    "        Rs = np.einsum('ijk->kij', np.reshape(np.repeat(np.repeat(payoffs, \n",
    "                                                                  self.N), \n",
    "                                                        self.M), \n",
    "                                              (self.N, self.M, self.N)))\n",
    "        deltas = Rs - Qs\n",
    "        # TODO: the cubes could be built at the beginning of the experiment\n",
    "        alphac_cube = np.reshape(np.repeat(self.alphac, self.N*self.M), \n",
    "                                 (self.N, self.N, self.M))\n",
    "        alphad_cube = np.reshape(np.repeat(self.alphad, self.N*self.M), \n",
    "                                 (self.N, self.N, self.M))\n",
    "        pos = deltas > 0\n",
    "        ags = np.arange(0, self.N, 1)\n",
    "        # choice_mask selects all actions that have been taken\n",
    "        choice_mask = np.zeros((self.N, self.N, self.M)).astype(bool)\n",
    "        choice_mask[:, ags, choices] = True\n",
    "        deltas[~choice_mask] = 0\n",
    "        # own_mask selects actions per agent\n",
    "        own_mask = np.zeros((self.N, self.N, self.M)).astype(bool)\n",
    "        own_mask[ags, :, choices] = True\n",
    "        # same_act selects actions similar to my actions\n",
    "        same_act = choice_mask & own_mask\n",
    "        # other_act selects actions different from mine\n",
    "        other_act = choice_mask & ~own_mask\n",
    "        same_pos = pos & same_act\n",
    "        other_pos = pos & other_act\n",
    "        same_neg = ~pos & same_act\n",
    "        other_neg = ~pos & other_act\n",
    "        alphas = np.zeros((self.N, self.N, self.M))\n",
    "        alphas[same_pos] = alphac_cube[same_pos]\n",
    "        alphas[same_neg] = alphad_cube[same_neg]\n",
    "        alphas[other_neg] = alphac_cube[other_neg]\n",
    "        alphas[other_pos] = alphad_cube[other_pos] \n",
    "        # obs_mask selects actions that I observe; \n",
    "        # TODO: could be implemented at the beginning of the experiment\n",
    "        obs_ij = np.asarray(nx.adjacency_matrix(G_att).todense()).astype(bool)\n",
    "        obs_mask = np.reshape(np.repeat(obs_ij, self.M), (self.N, self.N, \n",
    "                                                          self.M))\n",
    "        np.fill_diagonal(obs_ij, True)\n",
    "        obs_mask[:, :, 0] = obs_ij\n",
    "        obs_mask[:, :, 1] = obs_ij\n",
    "        deltas[~obs_mask] = 0\n",
    "        deltas *= alphas\n",
    "        deltas_sum = np.sum(deltas, axis=1)\n",
    "        Qtable += deltas_sum\n",
    "        return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f4cb29",
   "metadata": {},
   "source": [
    "## Example agents\n",
    "\n",
    "To illustrate the methods, let's consider 2 example agents. Both have a confirmation bias. Their parameters are the following:\n",
    "\n",
    "|         | $\\alpha_C$ | $\\alpha_D$ | $\\beta$ |\n",
    "| --------| ---------- | ---------- | ------- |\n",
    "| agent 1 | $0.2$      | $0.1$      | $5.$     |\n",
    "| agent 2 | $0.15$     | $0.05$     | $4.$     |\n",
    "\n",
    "* $\\alpha_C$: confirmatory learning rate\n",
    "* $\\alpha_D$: disconfirmatory learning rate\n",
    "* $\\beta$: inverse temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962dd65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example agents\n",
    "params = np.array([[0.2, 0.1, 5.],  # agent 1's params\n",
    "                   [0.15, 0.05, 4.]])  # agent 2's params\n",
    "\n",
    "example = SimpleConf(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c0a12",
   "metadata": {},
   "source": [
    "## SimpleConf methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa189545",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SimpleConf.connect_agents_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19866cb",
   "metadata": {},
   "source": [
    "**Input**:\n",
    "\n",
    "* None\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* Fully connected, non-directed \"attention graph\": each agent pays attention to all others.\n",
    "\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ab62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting example agents\n",
    "G = example.connect_agents_full()\n",
    "\n",
    "# Draw graph\n",
    "plt.figure(figsize=(2.5, 1.5))\n",
    "nx.draw(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SimpleConf.compute_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40054d36",
   "metadata": {},
   "source": [
    "**Input**: \n",
    "* `Qtable`: Q-table, 2d-array: *number of agents* $\\times$ *number of options*\n",
    "\n",
    "|         | option 1  | option 2  |\n",
    "| --------| --------- | --------- |\n",
    "| agent 1 | $Q_{1,1}$ | $Q_{1,2}$ |\n",
    "| agent 2 | $Q_{2,1}$ | $Q_{2,2}$ |\n",
    "\n",
    "\n",
    "**Output**:\n",
    "* `Ptable`: P-table, 2d-array: *number of agents* $\\times$ *number of options*\n",
    "\n",
    "|         | option 1  | option 2  |\n",
    "| --------| --------- | --------- |\n",
    "| agent 1 | $P_{1,1}$ | $P_{1,2}$ |\n",
    "| agent 2 | $P_{2,1}$ | $P_{2,2}$ |\n",
    "\n",
    "\n",
    "\n",
    "#### Softmax policy\n",
    "\n",
    "Probability that agent $i$ chooses option $j$ given Q-values $Q_{i,k}$, for $k$ any available option:\n",
    "\n",
    "$$ P_{i,j} = \\frac{exp(\\beta_{i} Q_{i,j})}{\\sum_{k} exp(\\beta_{i} Q_{i,k})} $$\n",
    "\n",
    "with $\\beta_{i}$ agent $i$'s inverse temperature.\n",
    "\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "With 2 agents, 2 options, all Qs are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58718d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qtable = np.zeros((2, 2))\n",
    "Ptable = example.compute_softmax(Qtable)\n",
    "Ptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SimpleConf.choose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400ffd4",
   "metadata": {},
   "source": [
    "**Input**:\n",
    "\n",
    "* `Ptable`: P-table, 2d-array: *number of agents* $\\times$ *number of options*\n",
    "\n",
    "|         | option 1  | option 2  |\n",
    "| --------| --------- | --------- |\n",
    "| agent 1 | $P_{1,1}$ | $P_{1,2}$ |\n",
    "| agent 2 | $P_{2,1}$ | $P_{2,2}$ |\n",
    "\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* `choices`: choice (i.e., chosen option) list, 1d-array: *number of agents*\n",
    "\n",
    "|         | choice  | \n",
    "| --------| ------- | \n",
    "| agent 1 | $c_{1}$ | \n",
    "| agent 2 | $c_{2}$ | \n",
    "\n",
    "\n",
    "NB: options are labelled $0$ to $M-1$, with $M$ number of available options.\n",
    "\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute example agents' choices according to previous Ptable\n",
    "choices = example.choose(Ptable)\n",
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1016be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: given option labelling, over many simulations, average\n",
    "# choice should approach probability of choosing option 1\n",
    "choices_test = np.zeros((2, 10000))\n",
    "for i in range(10000):  # loop over simulations\n",
    "    choices_test[:, i] = example.choose(Ptable)\n",
    "    \n",
    "np.mean(choices_test, axis=1)  # compute average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SimpleConf.all_take_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59270c",
   "metadata": {},
   "source": [
    "**Input**:\n",
    "\n",
    "* `Qtable`: Q-table, 2d-array: *number of agents* $\\times$ *number of options*\n",
    "\n",
    "|         | option 1  | option 2  |\n",
    "| --------| --------- | --------- |\n",
    "| agent 1 | $Q_{1,1}$ | $Q_{1,2}$ |\n",
    "| agent 2 | $Q_{2,1}$ | $Q_{2,2}$ |\n",
    "\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* `choices`: choice (i.e., chosen option) list, 1d-array: *number of agents*\n",
    "\n",
    "|         | choice  | \n",
    "| --------| ------- | \n",
    "| agent 1 | $c_{1}$ | \n",
    "| agent 2 | $c_{2}$ | \n",
    "\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute example agents' choices according to previous Qtable\n",
    "choices = example.all_take_action(Qtable)\n",
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b0c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: given option labelling, over many simulations, average\n",
    "# choice should approach probability of choosing option 1\n",
    "choices_test = np.zeros((2, 10000))\n",
    "for i in range(10000):  # loop over simulations\n",
    "    choices_test[:, i] = example.all_take_action(Qtable)\n",
    "    \n",
    "np.mean(choices_test, axis=1)  # compute average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f5741",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SimpleConf.update_Qvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a430a",
   "metadata": {},
   "source": [
    "**Input**:\n",
    "\n",
    "* `G_att`: attention graph, obtained through `connect_agents_full`\n",
    "* `choices`: choice (i.e., chosen option) list, 1d-array: *number of agents*\n",
    "\n",
    "|         | choice  | \n",
    "| --------| ------- | \n",
    "| agent 1 | $c_{1}$ | \n",
    "| agent 2 | $c_{2}$ | \n",
    "\n",
    "* `payoffs`: payoff list returned by task, 1d-array: *number of agents*\n",
    "\n",
    "|         | payoff  | \n",
    "| --------| ------- | \n",
    "| agent 1 | $r_{1}$ | \n",
    "| agent 2 | $r_{2}$ | \n",
    "\n",
    "* `Qtable`: Q-table, 2d-array: *number of agents* $\\times$ *number of options*\n",
    "\n",
    "|         | option 1  | option 2  |\n",
    "| --------| --------- | --------- |\n",
    "| agent 1 | $Q_{1,1}$ | $Q_{1,2}$ |\n",
    "| agent 2 | $Q_{2,1}$ | $Q_{2,2}$ |\n",
    "\n",
    "\n",
    "\n",
    "**Output**:\n",
    "* `Qtable`: updated Q-table, 2d-array: *number of agents* $\\times$ *number of options*\n",
    "\n",
    "|         | option 1  | option 2  |\n",
    "| --------| --------- | --------- |\n",
    "| agent 1 | $Q_{1,1}$ | $Q_{1,2}$ |\n",
    "| agent 2 | $Q_{2,1}$ | $Q_{2,2}$ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
