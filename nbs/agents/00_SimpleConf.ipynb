{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85685c1",
   "metadata": {},
   "source": [
    "# SimpleConf\n",
    "\n",
    "> Performs updating of state-action values according to a simple Rescorla-Wagner rule, with use of social information. Allows for asymmetric learning rates in the form of a confirmation/disconfirmation bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e558e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents/SimpleConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "\n",
    "# Imports for examples\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec595d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SimpleConf(object):\n",
    "    \"\"\"\n",
    "    Class for simple reinforcement learning (Rescorla-Wagner rule)\n",
    "    with confirmation/disconfirmation bias.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 params: np.ndarray):  # agents' parameters\n",
    "        \n",
    "        self.N = np.shape(params)[0]  # number of agents\n",
    "        self.M = 2  # number of options\n",
    "        self.alphac = params[:, 0]   # confirmatory learning rates\n",
    "        self.alphad = params[:, 1]   # disconfirmatory learning rates\n",
    "        self.beta = params[:, 2]    # inverse temperatures\n",
    "\n",
    "    def connect_agents_full(self):\n",
    "        \"\"\"Connects agents according to a fully connected graph.\"\"\"\n",
    "        return nx.complete_graph(self.N)\n",
    "\n",
    "    def compute_softmax(self, Qtable):\n",
    "        \"\"\"Returns a probability table for all agents for all actions,\n",
    "        from agents' Qtable.\"\"\"\n",
    "        beta = np.row_stack(self.beta)\n",
    "        num = np.exp(beta*Qtable)  # numerator\n",
    "        den = np.sum(num, axis=1)  # denominator\n",
    "        return num/den[:, None]\n",
    "\n",
    "    def choose(self, Ptable):\n",
    "        \"\"\"Computes chosen options from agents's probability table.\"\"\"\n",
    "        choices = np.zeros((np.shape(Ptable)[0]))  # 1 choice per agent\n",
    "        rd = np.reshape(np.random.rand(len(choices)), (len(choices), 1))\n",
    "        # a random number between 0 and 1 is attributed to each agent\n",
    "        choices = np.sum(rd > np.cumsum(Ptable, axis=1), axis=1) # if \n",
    "        # random number is smaller than agent's probability of choosing 0,\n",
    "        # then agent chooses 0\n",
    "        choices = choices.astype(int)  # converts the choices to int values\n",
    "        return choices\n",
    "    \n",
    "    def all_take_action(self, Qtable):\n",
    "        \"\"\"Computes all agents' choices from their Qtable.\n",
    "        Combines `compute_softmax` and `choose`.\n",
    "        \"\"\"\n",
    "        Ptable = self.compute_softmax(Qtable)  # converts Q-values to\n",
    "        # policy via softmax function\n",
    "        choices = self.choose(Ptable)  # choose an option according to\n",
    "        # policy\n",
    "        return choices\n",
    "    \n",
    "    def update_Qvalues(self, G_att, choices, payoffs, Qtable): \n",
    "        \"\"\"Updates all agents' Q-values according to CARL.\"\"\"\n",
    "        Qs = np.einsum('ijk->ikj', np.reshape(np.repeat(Qtable, self.N), \n",
    "                                              (self.N, self.M, self.N)))\n",
    "        Rs = np.einsum('ijk->kij', np.reshape(np.repeat(np.repeat(payoffs, \n",
    "                                                                  self.N), \n",
    "                                                        self.M), \n",
    "                                              (self.N, self.M, self.N)))\n",
    "        # dimensions: own Q-values, options, others' actions\n",
    "        deltas = Rs - Qs  # prediction errors\n",
    "        alphac_cube = np.reshape(np.repeat(self.alphac, self.N*self.M), \n",
    "                                 (self.N, self.N, self.M))\n",
    "        alphad_cube = np.reshape(np.repeat(self.alphad, self.N*self.M), \n",
    "                                 (self.N, self.N, self.M))\n",
    "        # 3d matrices with only confirmatory alphas vs. disconfirmatory\n",
    "        # alphas\n",
    "        pos = deltas > 0  # mask that indicates where pred errors are \n",
    "        # positive\n",
    "        ags = np.arange(0, self.N, 1)  # vector of agent indices\n",
    "        # choice_mask selects all actions that have been taken\n",
    "        choice_mask = np.zeros((self.N, self.N, self.M)).astype(bool)\n",
    "        choice_mask[:, ags, choices] = True\n",
    "        deltas[~choice_mask] = 0\n",
    "        # own_mask selects actions per agent\n",
    "        own_mask = np.zeros((self.N, self.N, self.M)).astype(bool)\n",
    "        own_mask[ags, :, choices] = True\n",
    "        # same_act selects actions similar to my actions\n",
    "        same_act = choice_mask & own_mask\n",
    "        # other_act selects actions different from mine\n",
    "        other_act = choice_mask & ~own_mask\n",
    "        same_pos = pos & same_act\n",
    "        other_pos = pos & other_act\n",
    "        same_neg = ~pos & same_act\n",
    "        other_neg = ~pos & other_act\n",
    "        # alphas determines which alpha to attribute to each pred error\n",
    "        alphas = np.zeros((self.N, self.N, self.M))\n",
    "        alphas[same_pos] = alphac_cube[same_pos]\n",
    "        alphas[same_neg] = alphad_cube[same_neg]\n",
    "        alphas[other_neg] = alphac_cube[other_neg]\n",
    "        alphas[other_pos] = alphad_cube[other_pos] \n",
    "        # obs_mask selects actions that I observe\n",
    "        obs_ij = np.asarray(nx.adjacency_matrix(G_att).todense()).astype(bool)\n",
    "        obs_mask = np.reshape(np.repeat(obs_ij, self.M), (self.N, self.N, \n",
    "                                                          self.M))\n",
    "        np.fill_diagonal(obs_ij, True)  # I observe my own actions\n",
    "        obs_mask[:, :, 0] = obs_ij\n",
    "        obs_mask[:, :, 1] = obs_ij\n",
    "        deltas[~obs_mask] = 0  # cancelled pred errors from unobserved actions\n",
    "        # update Q-values according to CARL model\n",
    "        deltas *= alphas\n",
    "        deltas_sum = np.sum(deltas, axis=1)\n",
    "        Qtable += deltas_sum\n",
    "        return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f4cb29",
   "metadata": {},
   "source": [
    "## Example agents\n",
    "\n",
    "To illustrate the methods, let's consider 2 example agents. Both have a confirmation bias. Their parameters are the following:\n",
    "\n",
    "|         | $\\alpha_C$ | $\\alpha_D$ | $\\beta$ |\n",
    "| --------| ---------- | ---------- | ------- |\n",
    "| agent 1 | $0.2$      | $0.1$      | $5.$     |\n",
    "| agent 2 | $0.15$     | $0.05$     | $4.$     |\n",
    "\n",
    "* $\\alpha_C$: confirmatory learning rate\n",
    "* $\\alpha_D$: disconfirmatory learning rate\n",
    "* $\\beta$: inverse temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962dd65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example agents\n",
    "params = np.array([[0.2, 0.1, 5.],  # agent 1's params\n",
    "                   [0.15, 0.05, 4.]])  # agent 2's params\n",
    "\n",
    "example = SimpleConf(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c0a12",
   "metadata": {},
   "source": [
    "## SimpleConf methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa189545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### SimpleConf.connect_agents_full\n",
       "\n",
       ">      SimpleConf.connect_agents_full ()\n",
       "\n",
       "Connects agents according to a fully connected graph."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### SimpleConf.connect_agents_full\n",
       "\n",
       ">      SimpleConf.connect_agents_full ()\n",
       "\n",
       "Connects agents according to a fully connected graph."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SimpleConf.connect_agents_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19866cb",
   "metadata": {},
   "source": [
    "**Input**:\n",
    "\n",
    "* None\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* Fully connected, non-directed \"attention graph\": each agent pays attention to all others.\n",
    "\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ab62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Connecting example agents\n",
    "G = example.connect_agents_full()\n",
    "\n",
    "# Draw graph\n",
    "plt.figure(figsize=(2.5, 1.5))\n",
    "nx.draw(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538176f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### SimpleConf.compute_softmax\n",
       "\n",
       ">      SimpleConf.compute_softmax (Qtable)\n",
       "\n",
       "Returns a probability table for all agents for all actions,\n",
       "from agents' Qtable."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### SimpleConf.compute_softmax\n",
       "\n",
       ">      SimpleConf.compute_softmax (Qtable)\n",
       "\n",
       "Returns a probability table for all agents for all actions,\n",
       "from agents' Qtable."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SimpleConf.compute_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40054d36",
   "metadata": {},
   "source": [
    "**Input**: \n",
    "* `Qtable`: Q-table, 2d-array: *number of agents* $\\times$ *number of options*\n",
    "\n",
    "|         | option 1  | option 2  |\n",
    "| --------| --------- | --------- |\n",
    "| agent 1 | $Q_{1,1}$ | $Q_{1,2}$ |\n",
    "| agent 2 | $Q_{2,1}$ | $Q_{2,2}$ |\n",
    "\n",
    "\n",
    "**Output**:\n",
    "* `Ptable`: P-table, 2d-array: *number of agents* $\\times$ *number of options*\n",
    "\n",
    "|         | option 1  | option 2  |\n",
    "| --------| --------- | --------- |\n",
    "| agent 1 | $P_{1,1}$ | $P_{1,2}$ |\n",
    "| agent 2 | $P_{2,1}$ | $P_{2,2}$ |\n",
    "\n",
    "\n",
    "\n",
    "#### Softmax policy\n",
    "\n",
    "Probability that agent $i$ chooses option $j$ given Q-values $Q_{i,k}$, for $k$ any available option:\n",
    "\n",
    "$$ P_{i,j} = \\frac{exp(\\beta_{i} Q_{i,j})}{\\sum_{k} exp(\\beta_{i} Q_{i,k})} $$\n",
    "\n",
    "with $\\beta_{i}$ agent $i$'s inverse temperature.\n",
    "\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "With 2 agents, 2 options, all Qs are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58718d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5],\n",
       "       [0.5, 0.5]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qtable = np.zeros((2, 2))\n",
    "Ptable = example.compute_softmax(Qtable)\n",
    "Ptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30c462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/clembergerot/CARL/blob/main/CARL/agents/SimpleConf.py#L38){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SimpleConf.choose\n",
       "\n",
       ">      SimpleConf.choose (Ptable)\n",
       "\n",
       "Computes chosen options from agents's probability table."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/clembergerot/CARL/blob/main/CARL/agents/SimpleConf.py#L38){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SimpleConf.choose\n",
       "\n",
       ">      SimpleConf.choose (Ptable)\n",
       "\n",
       "Computes chosen options from agents's probability table."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SimpleConf.choose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400ffd4",
   "metadata": {},
   "source": [
    "**Input**:\n",
    "\n",
    "* `Ptable`: P-table, 2d-array: *number of agents* $\\times$ *number of options*\n",
    "\n",
    "|         | option 1  | option 2  |\n",
    "| --------| --------- | --------- |\n",
    "| agent 1 | $P_{1,1}$ | $P_{1,2}$ |\n",
    "| agent 2 | $P_{2,1}$ | $P_{2,2}$ |\n",
    "\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* `choices`: choice (i.e., chosen option) list, 1d-array: *number of agents*\n",
    "\n",
    "|         | choice  | \n",
    "| --------| ------- | \n",
    "| agent 1 | $c_{1}$ | \n",
    "| agent 2 | $c_{2}$ | \n",
    "\n",
    "\n",
    "NB: options are labelled $0$ to $M-1$, with $M$ number of available options.\n",
    "\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c4ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute example agents' choices according to previous Ptable\n",
    "choices = example.choose(Ptable)\n",
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1016be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5149, 0.5091])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test: given option labelling, over many simulations, average\n",
    "# choice should approach probability of choosing option 1\n",
    "choices_test = np.zeros((2, 10000))\n",
    "for i in range(10000):  # loop over simulations\n",
    "    choices_test[:, i] = example.choose(Ptable)\n",
    "    \n",
    "np.mean(choices_test, axis=1)  # compute average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720cfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/clembergerot/CARL/blob/main/CARL/agents/SimpleConf.py#L46){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SimpleConf.all_take_action\n",
       "\n",
       ">      SimpleConf.all_take_action (Qtable)\n",
       "\n",
       "Computes all agents' choices from their Qtable.\n",
       "Combines `compute_softmax` and `choose`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/clembergerot/CARL/blob/main/CARL/agents/SimpleConf.py#L46){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SimpleConf.all_take_action\n",
       "\n",
       ">      SimpleConf.all_take_action (Qtable)\n",
       "\n",
       "Computes all agents' choices from their Qtable.\n",
       "Combines `compute_softmax` and `choose`."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SimpleConf.all_take_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59270c",
   "metadata": {},
   "source": [
    "**Input**:\n",
    "\n",
    "* `Qtable`: Q-table, 2d-array: *number of agents* $\\times$ *number of options*\n",
    "\n",
    "|         | option 1  | option 2  |\n",
    "| --------| --------- | --------- |\n",
    "| agent 1 | $Q_{1,1}$ | $Q_{1,2}$ |\n",
    "| agent 2 | $Q_{2,1}$ | $Q_{2,2}$ |\n",
    "\n",
    "\n",
    "**Output**:\n",
    "\n",
    "* `choices`: choice (i.e., chosen option) list, 1d-array: *number of agents*\n",
    "\n",
    "|         | choice  | \n",
    "| --------| ------- | \n",
    "| agent 1 | $c_{1}$ | \n",
    "| agent 2 | $c_{2}$ | \n",
    "\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9239b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute example agents' choices according to previous Qtable\n",
    "choices = example.all_take_action(Qtable)\n",
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b0c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.509 , 0.4981])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test: given option labelling, over many simulations, average\n",
    "# choice should approach probability of choosing option 1\n",
    "choices_test = np.zeros((2, 10000))\n",
    "for i in range(10000):  # loop over simulations\n",
    "    choices_test[:, i] = example.all_take_action(Qtable)\n",
    "    \n",
    "np.mean(choices_test, axis=1)  # compute average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f5741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/clembergerot/CARL/blob/main/CARL/agents/SimpleConf.py#L54){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SimpleConf.update_Qvalues\n",
       "\n",
       ">      SimpleConf.update_Qvalues (G_att, choices, payoffs, Qtable)\n",
       "\n",
       "Updates all agents' Q-values according to CARL."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/clembergerot/CARL/blob/main/CARL/agents/SimpleConf.py#L54){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SimpleConf.update_Qvalues\n",
       "\n",
       ">      SimpleConf.update_Qvalues (G_att, choices, payoffs, Qtable)\n",
       "\n",
       "Updates all agents' Q-values according to CARL."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SimpleConf.update_Qvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a430a",
   "metadata": {},
   "source": [
    "**Input**:\n",
    "\n",
    "* `G_att`: attention graph, obtained through `connect_agents_full`\n",
    "* `choices`: choice (i.e., chosen option) list, 1d-array: *number of agents*\n",
    "\n",
    "|         | choice  | \n",
    "| --------| ------- | \n",
    "| agent 1 | $c_{1}$ | \n",
    "| agent 2 | $c_{2}$ | \n",
    "\n",
    "* `payoffs`: payoff list returned by task, 1d-array: *number of agents*\n",
    "\n",
    "|         | payoff  | \n",
    "| --------| ------- | \n",
    "| agent 1 | $r_{1}$ | \n",
    "| agent 2 | $r_{2}$ | \n",
    "\n",
    "* `Qtable`: Q-table, 2d-array: *number of agents* $\\times$ *number of options*\n",
    "\n",
    "|         | option 1  | option 2  |\n",
    "| --------| --------- | --------- |\n",
    "| agent 1 | $Q_{1,1}$ | $Q_{1,2}$ |\n",
    "| agent 2 | $Q_{2,1}$ | $Q_{2,2}$ |\n",
    "\n",
    "\n",
    "\n",
    "**Output**:\n",
    "* `Qtable`: updated Q-table, 2d-array: *number of agents* $\\times$ *number of options*\n",
    "\n",
    "|         | option 1  | option 2  |\n",
    "| --------| --------- | --------- |\n",
    "| agent 1 | $Q_{1,1}$ | $Q_{1,2}$ |\n",
    "| agent 2 | $Q_{2,1}$ | $Q_{2,2}$ |\n",
    "\n",
    "\n",
    "**Example**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167da40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1 ,  0.1 ],\n",
       "       [-0.15,  0.15]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update previous Q-values given following payoffs:\n",
    "payoffs = np.array([-1, 1])  # first agent got -1, second got 1\n",
    "example.update_Qvalues(G, choices, payoffs, Qtable)  # update Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
