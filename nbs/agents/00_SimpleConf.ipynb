{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85685c1",
   "metadata": {},
   "source": [
    "# SimpleConf\n",
    "\n",
    "> Performs updating of state-action values according to a simple Rescorla-Wagner rule. Allows for asymmetric learning rates in the form of a confirmation/disconfirmation bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e558e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agents/SimpleConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec595d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SimpleConf(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.N = np.shape(params)[0]   # number of agents\n",
    "        self.M = 2  # number of options\n",
    "        self.alphac = params[:, 0]   # confirmatory learning rates\n",
    "        self.alphad = params[:, 1]   # disconfirmatory learning rates\n",
    "        self.beta = params[:, 2]    # inverse temperatures\n",
    "\n",
    "    def connect_agents_full(self):\n",
    "        # all agents have full attention capacity\n",
    "        return nx.complete_graph(self.N)\n",
    "\n",
    "    def compute_softmax(self, Qtable):\n",
    "        # returns a probability table for all agents for all actions\n",
    "        beta = np.row_stack(self.beta)\n",
    "        num = np.exp(beta*Qtable)  # numerator\n",
    "        den = np.sum(num, axis=1)  # denominator\n",
    "        return num/den[:, None]\n",
    "\n",
    "    def choose(self, Ptable):\n",
    "        # computes chosen options from probability table\n",
    "        choices = np.zeros((np.shape(Ptable)[0]))  # 1 choice per agent\n",
    "        rd = np.reshape(np.random.rand(len(choices)), (len(choices), 1))\n",
    "        choices = np.sum(rd > np.cumsum(Ptable, axis=1), axis=1)\n",
    "        choices = choices.astype(int)  # converts the choices to int values\n",
    "        return choices\n",
    "\n",
    "    def track_payoffs(self, agent, G_att, choices, payoffs):\n",
    "        # tracks the choices and payoffs of oneself + neighbors\n",
    "        obs_choices = [choices[agent]] + [\n",
    "            choices[n] for n in list(G_att.neighbors(agent))]\n",
    "        # observed choices\n",
    "        obs_choices = list(dict.fromkeys(obs_choices))  # removes duplicates\n",
    "        obs_payoffs = [payoffs[obs] for obs in obs_choices]  # lists\n",
    "        # corresponding payoffs\n",
    "        trackmat = np.column_stack((obs_choices, obs_payoffs))  # matches\n",
    "        # tracked choices + payoffs in a matrix\n",
    "        return trackmat\n",
    "\n",
    "    def update_Qvalues_async(self, agent, G_att, choices, payoffs, Qtable):\n",
    "        c = max(choices)\n",
    "        delta_mat = np.zeros((self.N, c+1))\n",
    "        delta_mat[agent, choices[agent]] = payoffs[agent] - Qtable[agent, choices[agent]]\n",
    "        M = list(G_att.neighbors(agent))\n",
    "        delta_mat[M, choices[M]] = payoffs[M] - Qtable[agent, choices[M]]\n",
    "        j = choices[agent]\n",
    "        delta_mat[:, j][delta_mat[:, j] > 0] *= self.alphac[agent]\n",
    "        delta_mat[:, j][delta_mat[:, j] <= 0] *= self.alphad[agent]\n",
    "        I = list(np.arange(0, c+1, 1))\n",
    "        I.remove(j)\n",
    "        delta_mat[:, I][delta_mat[:, I] > 0] *= self.alphad[agent]\n",
    "        delta_mat[:, I][delta_mat[:, I] <= 0] *= self.alphac[agent]\n",
    "        inc = np.sum(delta_mat, axis=0)\n",
    "        Qtable[agent, :c+1] += inc\n",
    "        return Qtable\n",
    "\n",
    "    def update_Qvalues(self, agent, trackmat, Qtable):\n",
    "        # updates Q-values according to a simple RW rule + asymmetric updating\n",
    "        trackmat_int = trackmat.astype(int)  # used for indexing\n",
    "        delta = trackmat[:, 1] - Qtable[agent, trackmat_int[:, 0]]  # all\n",
    "        # prediction errors\n",
    "        alpha = trackmat[:, 1] - Qtable[agent, trackmat_int[:, 0]]  # building\n",
    "        # alpha from prediction errors\n",
    "        alpha[alpha > 0] = self.alphad[agent]  # delta will be updated with\n",
    "        # alphad if positive\n",
    "        alpha[alpha <= 0] = self.alphac[agent]  # and with alphac if negative\n",
    "        # this is reversed when choice is own choice\n",
    "        if delta[0] >= 0:\n",
    "            alpha[0] = self.alphac[agent]\n",
    "        else:\n",
    "            alpha[0] = self.alphad[agent]\n",
    "        Qtable[agent, trackmat_int[:, 0]] += alpha * delta  # updates all\n",
    "        # Q-values\n",
    "        return Qtable\n",
    "    \n",
    "    # Allows to update Q-values for all agents without for loops\n",
    "    def update_Qvalues_async2(self, G_att, choices, payoffs, Qtable): \n",
    "        Qs = np.einsum('ijk->ikj', np.reshape(np.repeat(Qtable, self.N), \n",
    "                                              (self.N, self.M, self.N)))\n",
    "        Rs = np.einsum('ijk->kij', np.reshape(np.repeat(np.repeat(payoffs, \n",
    "                                                                  self.N), \n",
    "                                                        self.M), \n",
    "                                              (self.N, self.M, self.N)))\n",
    "        deltas = Rs - Qs\n",
    "        # TODO: the cubes could be built at the beginning of the experiment\n",
    "        alphac_cube = np.reshape(np.repeat(self.alphac, self.N*self.M), \n",
    "                                 (self.N, self.N, self.M))\n",
    "        alphad_cube = np.reshape(np.repeat(self.alphad, self.N*self.M), \n",
    "                                 (self.N, self.N, self.M))\n",
    "        pos = deltas > 0\n",
    "        ags = np.arange(0, self.N, 1)\n",
    "        # choice_mask selects all actions that have been taken\n",
    "        choice_mask = np.zeros((self.N, self.N, self.M)).astype(bool)\n",
    "        choice_mask[:, ags, choices] = True\n",
    "        deltas[~choice_mask] = 0\n",
    "        # own_mask selects actions per agent\n",
    "        own_mask = np.zeros((self.N, self.N, self.M)).astype(bool)\n",
    "        own_mask[ags, :, choices] = True\n",
    "        # same_act selects actions similar to my actions\n",
    "        same_act = choice_mask & own_mask\n",
    "        # other_act selects actions different from mine\n",
    "        other_act = choice_mask & ~own_mask\n",
    "        same_pos = pos & same_act\n",
    "        other_pos = pos & other_act\n",
    "        same_neg = ~pos & same_act\n",
    "        other_neg = ~pos & other_act\n",
    "        alphas = np.zeros((self.N, self.N, self.M))\n",
    "        alphas[same_pos] = alphac_cube[same_pos]\n",
    "        alphas[same_neg] = alphad_cube[same_neg]\n",
    "        alphas[other_neg] = alphac_cube[other_neg]\n",
    "        alphas[other_pos] = alphad_cube[other_pos] \n",
    "        # obs_mask selects actions that I observe; \n",
    "        # TODO: could be implemented at the beginning of the experiment\n",
    "        obs_ij = np.asarray(nx.adjacency_matrix(G_att).todense()).astype(bool)\n",
    "        obs_mask = np.reshape(np.repeat(obs_ij, self.M), (self.N, self.N, \n",
    "                                                          self.M))\n",
    "        np.fill_diagonal(obs_ij, True)\n",
    "        obs_mask[:, :, 0] = obs_ij\n",
    "        obs_mask[:, :, 1] = obs_ij\n",
    "        deltas[~obs_mask] = 0\n",
    "        deltas *= alphas\n",
    "        deltas_sum = np.sum(deltas, axis=1)\n",
    "        Qtable += deltas_sum\n",
    "        return Qtable\n",
    "\n",
    "\n",
    "    def all_take_action(self, Qtable):\n",
    "        # all agents choose an option according to their Q-tables\n",
    "        Ptable = self.compute_softmax(Qtable)\n",
    "        choices = self.choose(Ptable)\n",
    "        return choices\n",
    "\n",
    "    def all_update_Q(self, Qtable, G_att, payoffs, choices):\n",
    "        # all agents update their Q-values\n",
    "        for agent in range(self.N):\n",
    "            trackmat = self.track_payoffs(agent, G_att, choices, payoffs)\n",
    "            Qtable = self.update_Qvalues(agent, trackmat, Qtable)\n",
    "        return Qtable\n",
    "\n",
    "    def all_update_Q_async(self, Qtable, G_att, payoffs, choices):\n",
    "        # all agents update their Q-values\n",
    "        for agent in range(self.N):\n",
    "            Qtable = self.update_Qvalues_async(agent, G_att, choices, payoffs,\n",
    "            Qtable)\n",
    "        return Qtable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
